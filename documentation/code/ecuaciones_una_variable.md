# Documentaci√≥n M√≥dulo Ecuaciones de una Variable

## 1- M√©todo de Bisecci√≥n (`bisection(f, a, b, TOL, N0)`)

El **m√©todo de bisecci√≥n** es un m√©todo num√©rico para encontrar la ra√≠z de una funci√≥n continua. Se basa en el **teorema del valor intermedio**, el cual establece que si una funci√≥n `f(x)` es continua en un intervalo `[a, b]` y `f(a)` y `f(b)` tienen signos opuestos, entonces existe al menos un punto c en `[a, b]` tal que `f(c) = 0`.

### Proceso del M√©todo

1. **Verificaci√≥n de la condici√≥n inicial:**  
   Se comprueba que `f(a)` y `f(b)` tengan signos opuestos, es decir, que `f(a) * f(b) < 0`.

2. **Inicializaci√≥n**  
   Se inicializa el contador en 1 y se eval√∫a `FA = f(a)`.

3. **Iteraci√≥n**  
   En cada iteraci√≥n se calcula el punto medio y se eval√∫a `FP = f(p)`.  
   Si `FP` es 0 o si la mitad del ancho del intervalo (b-a)/2 es menor que la tolerancia, se retorna `p` junto con el n√∫mero de iteraciones.

4. **Actualizaci√≥n del Intervalo**  
   Se incrementa i y se decide:
   - Si `FA` y `FP` tienen signos opuestos, la ra√≠z se encuentra en `[ùëé,p]` por lo que se actualiza `b = p`.
   - De lo contrario, la ra√≠z se encuentra en `[p,ùëè]` por lo que se actualiza `a = p` y `FA = FP`.

5. **Fallo**  
   Si se alcanzan `N0` iteraciones sin convergencia, se lanza un error.

### Parametros de Entrada y Salida
```python
def bisection(f, a, b, TOL=1e-5, N0=100):
    """
    Entradas:
      f        : funci√≥n (callable)
      a, b     : Puntos finales del intervalo en el que se busca la ra√≠z.
      TOL      : Tolerancia para la aproximaci√≥n de la ra√≠z.
      N0       : N√∫mero m√°ximo de iteraciones permitidas.

    Salida:
      Retorna una tupla (p, i) donde:
         - p es la aproximaci√≥n a la ra√≠z.
         - i es el n√∫mero de iteraciones realizadas.
    """
```
### Ejemplo de Uso
Supongamos que queremos encontrar la ra√≠z de la funci√≥n `ùëì(ùë•)=ùë•**2‚àí4` en el intervalo `[0,3]`. Para ello, definimos la funci√≥n y llamamos a bisection_method:

```python
# Definici√≥n de la funci√≥n a analizar
def f(x):
    return x**2 - 4

# Llamada al m√©todo de bisecci√≥n en el intervalo [0, 3]
raiz, iteraciones = bisection_method(f, 0, 3)

print(f"La ra√≠z aproximada es {raiz} encontrada en {iteraciones} iteraciones.")
```

## 2- M√©todo de Iteraci√≥n de Punto Fijo (`fixed_point_iteration(g, p0, TOL, N0)`)

El **m√©todo de iteraci√≥n de punto fijo** es un m√©todo num√©rico para encontrar una soluci√≥n a la ecuaci√≥n `p = g(p)`. La idea es partir de una aproximaci√≥n inicial y generar una sucesi√≥n definida por `p{i+1} = g(pi)`.

Si la funci√≥n `g(p)` es contractiva en la regi√≥n cercana al punto fijo, la sucesi√≥n converge a un valor `p` tal que `p = g(p)`.

### Proceso del M√©todo

1. **Conversi√≥n**  
   Se convierte la funci√≥n dada a la forma `p = g(p)`.

2. **Inicializaci√≥n**   
  Se inicializa el contador de iteraciones y se inicia un bucle que se ejecuta mientras el n√∫mero de iteraciones no supere `N0`.

3. **Proceso**  
  Se calcula el siguiente valor `p` aplicando la funci√≥n `g` a la aproximaci√≥n actual `p0`.  
  Se verifica si la diferencia entre la nueva aproximaci√≥n y la anterior es menor que la tolerancia. Si es as√≠, se retorna `p` y el n√∫mero de iteraciones y se termina la ejecuci√≥n.

4. **Iteraci√≥n**   
  Se incrementa el contador de iteraciones. y se actualiza `p0` con el valor de `p` para la siguiente iteraci√≥n.

5. **Fallo**  
  Si se sale del bucle sin convergencia, se lanza un `ValueError` indicando que el m√©todo fall√≥ tras `N0` iteraciones.

### Parametros de Entrada y Salida

```python
def fixed_point_iteration(g, p0, TOL=1e-5, N0=100):
    """
    Entradas:
      g   : funci√≥n (callable) que define el m√©todo de punto fijo, es decir, g(p).
      p0  : aproximaci√≥n inicial.
      TOL : tolerancia para la convergencia.
      N0  : n√∫mero m√°ximo de iteraciones permitidas.
    
    Salida:
      Retorna una tupla (p, i) donde:
         - p es la aproximaci√≥n a la soluci√≥n.
         - i es el n√∫mero de iteraciones realizadas.
    """
   ```

### Ejemplo de Uso
Definimos la funci√≥n `g(p)` para el problema: `p = cos(p)`

   ```python
   def g(p):
        return math.cos(p)

    # Aproximaci√≥n inicial
    p0 = 1.0

    try:
        solution, iterations = fixed_point_iteration(g, p0, TOL=1e-5, N0=100)
        print(f"La soluci√≥n encontrada es {solution} en {iterations} iteraciones.")
    except ValueError as e:
        print(e)
   ```
## 3- M√©todo de Newton (`newton_method(f, p0, TOL=1e-5, N0=100, factor=1e-8)`)

El M√©todo de Newton es un procedimiento iterativo para encontrar soluciones aproximadas de la ecuaci√≥n `f(x)=0`. Se parte de una aproximaci√≥n inicial y se va mejorando dicha aproximaci√≥n usando la f√≥rmula `p = p0 - f(p0)/f‚Ä≤(p0)`

### Proceso del M√©todo

1. **Inicializaci√≥n**  
   Se elige una aproximaci√≥n inicial para la ra√≠z de la funci√≥n `f(x)=0`, se establece el contador de iteraciones en 1 y llamamos al metodo privado `_derivative` para calcular la derivada de la funci√≥n.

2. **Iteraci√≥n**  
   En cada iteraci√≥n se realiza lo siguiente:
   - **C√°lculo de la nueva aproximaci√≥n:**  
     Se calcula:
     `p= p0 ‚àí f(p0)/f‚Ä≤(p0)`, en nuestro caso `p = p0 - f_val / df_val`

     `df_val` se obtiene num√©ricamente mediante el m√©todo de diferencia centrada.
   - **Evaluaci√≥n del cambio:**  
     Se calcula la diferencia para evaluar el progreso de la iteraci√≥n.

   **`threshold`**:   El umbral se define como `factor * (1 + |p0|)`. Esto permite ajustar el umbral en funci√≥n de la escala de la aproximaci√≥n inicial. Si la magnitud de la derivada en `p0` es menor que este umbral, se considera que la derivada es demasiado peque√±a, y por tanto no es seguro aplicar el m√©todo de Newton.
3. **Criterio de Convergencia**  
   Si la diferencia es menor que la tolerancia predefinida, se considera que la iteraci√≥n ha convergido y se retorna la soluci√≥n aproximada junto con el n√∫mero de iteraciones.

4. **Actualizaci√≥n**  
   Si la condici√≥n de convergencia no se cumple, se incrementa el contador en 1 y se actualiza `p0` con el valor de `p` para la siguiente iteraci√≥n.

5. **Fallo**  
   Si la derivada es demasiado peque√±a en `p0` (es decir, menor que el umbral relativo calculado) o si se alcanza el n√∫mero m√°ximo de iteraciones sin convergencia, se lanza un ValueError indicando que el m√©todo de Newton no se puede aplicar o que ha fallado.

### Parametros de Entrada y Salida

```python
def newton_method(f, p0, TOL=1e-5, N0=100, factor=1e-8):
   """
      Entradas:
         f        : funci√≥n (callable) para la cual se busca la ra√≠z, f(x)=0.
         p0       : aproximaci√≥n inicial.
         TOL      : tolerancia para la convergencia.
         N0       : n√∫mero m√°ximo de iteraciones permitidas.
         factor   : factor para calcular el umbral relativo utilizado en la verificaci√≥n de la derivada.
            
      Salida:
         Retorna una tupla (p, i) donde:
            - p es la aproximaci√≥n a la ra√≠z.
            - i es el n√∫mero de iteraciones realizadas.
      """
```

### Ejemplo de Uso
Utilizamos el m√©todo de Newton, usando derivaci√≥n num√©rica, y elegimos una aproximaci√≥n inicial razonable.

   ```python

   def f(x):
      return x**3 - 2

   p0 = 1.5  
   solution, iterations = newton_method(f, p0, TOL=1e-5, N0=100, factor=1e-8)
   print(f"La soluci√≥n encontrada es {solution} en {iterations} iteraciones.")
   ```
## 4- M√©todo de la secante (`secant_method(f, p0, p1, TOL=1e-5, N0=100)`)

El **M√©todo de la Secante** es un m√©todo num√©rico para encontrar una soluci√≥n aproximada a la ecuaci√≥n `f(x)=0` sin requerir el c√°lculo expl√≠cito de la derivada. En lugar de usar la derivada, utiliza dos aproximaciones iniciales y calcula la siguiente aproximaci√≥n mediante la recta secante que une los puntos `(p0, f(p0))` y `(p1, f(p1))`.

### Proceso del M√©todo

1. **Inicializaci√≥n**  
   Se disponen dos aproximaciones iniciales `p0` y `p1` y se calcula:
   - `q0 = f(p0)`
   - `q1 = f(p1)`
   Se establece el contador de iteraciones en `i = 2`, ya que ya se conocen dos puntos.

2. **Iteraci√≥n**  
   Mientras que la iteraci√≥n es menor o igual al n√∫mero m√°ximo de iteraciones, se hacen los siguientes pasos:
   - **C√°lculo de la nueva aproximaci√≥n:**  
     Se calcula: `p = p1 - q1 * (p1 - p0) / (q1 - q0)`
   - **Criterio de Convergencia:**  
     Si `|p - p_1| < TOL`, se retorna `p` junto con el n√∫mero de iteraciones.
   - **Actualizaci√≥n:**  
     Se actualizan las variables para la siguiente iteraci√≥n:
     - `p0 <- p1` y `q0 <- q1`
     - `p1 <- p` y `q1 <- f(p)` Se incrementa `i`.

3. **Fallo**  
   Si se alcanzan `N0` iteraciones sin satisfacer el criterio de convergencia, se lanza un error indicando que el m√©todo no fue exitoso.

### Parametros de Entrada y Salida

```python
def secant_method(f, p0, p1, TOL=1e-5, N0=100):
    """
    Entradas:
      f    : funci√≥n (callable) para la cual se busca la ra√≠z, f(x)=0.
      p0   : primera aproximaci√≥n inicial.
      p1   : segunda aproximaci√≥n inicial.
      TOL  : tolerancia para la convergencia.
      N0   : n√∫mero m√°ximo de iteraciones permitidas.
    
    Salida:
      Retorna una tupla (p, i) donde:
         - p es la aproximaci√≥n a la ra√≠z.
         - i es el n√∫mero de iteraciones realizadas.
```

### Ejemplo de Uso

Considera la ecuaci√≥n `f(x)=x^2-2=0`. La ra√≠z real es `sqrt{2}, approx 1.41421`. Utilizaremos dos aproximaciones iniciales, por ejemplo, `p0=1` y `p1=2`.

```python

def f(x):
    return x**2 - 2

p0 = 1.0
p1 = 2.0

solution, iterations = secant_method(f, p0, p1, TOL=1e-5, N0=100)
print(f"La soluci√≥n encontrada es {solution} en {iterations} iteraciones.")
```
## 5- M√©todo de Posici√≥n Falsa (`false_position(f, p0, p1, TOL=1e-5, N0=100)`)

El **m√©todo de la posici√≥n falsa** es un m√©todo num√©rico para encontrar una soluci√≥n aproximada de la ecuaci√≥n `f(x)=0` cuando la funci√≥n `f` es continua en el intervalo `[p0, p1]` y `f(p0)` y `f(p1)` tienen signos opuestos. La idea es usar una recta secante (la l√≠nea que une los puntos `(p0, f(p0))` y `(p1, f(p1))`) para estimar la ra√≠z.

### Proceso del M√©todo

1. **Inicializaci√≥n**

   Se establecen dos aproximaciones iniciales.   
   Se calculan `q0 = f(p0)` y `q1 = f(p1)`.   
   Se fija el contador `i=2`.

2. **Iteraci√≥n**

   Se calcula una nueva aproximaci√≥n usando la f√≥rmula de la posici√≥n falsa: `p = p1 - q1 * (p1 - p0) / (q1 - q0)`.   
   Si `|p - p_1| < TOL`, se considera que el proceso ha convergido y se retorna `p` junto con el n√∫mero de iteraciones.
   Se incrementa el contador en 1.
   Se calcula `q = f(p)`. Luego, se verifica el signo de `q` respecto a `q1`:
     - Si `q * q1 < 0`, se actualiza `p0 = p1` y `q0 = q1`.
   Se actualiza `p1 = p` y `q1 = q`.
   
3. **Fallo**

   Si se alcanza el n√∫mero m√°ximo de iteraciones `N0` sin convergencia, se lanza un error indicando que el m√©todo fall√≥.

### Parametros de Entrada y Salida

```python
def false_position(f, p0, p1, TOL=1e-5, N0=100):
    """
    Entradas:
      f    : funci√≥n (callable) para la cual se busca la ra√≠z, f(x)=0.
      p0   : primera aproximaci√≥n inicial.
      p1   : segunda aproximaci√≥n inicial.
      TOL  : tolerancia para la convergencia.
      N0   : n√∫mero m√°ximo de iteraciones permitidas.
      
    Salida:
      Retorna una tupla (p, i) donde:
         - p es la aproximaci√≥n a la ra√≠z.
         - i es el n√∫mero de iteraciones realizadas.
   """
```

### Diferencia con Metodo de la Secante

Ambos m√©todos utilizan la idea de aproximar la ra√≠z mediante la intersecci√≥n de la l√≠nea secante que une dos puntos en la gr√°fica de `f(x)`, pero difieren en c√≥mo actualizan sus aproximaciones:

- **M√©todo de la Secante**  
  - Utiliza los dos √∫ltimos puntos calculados (sin mantener necesariamente una condici√≥n de bracketing) para generar la nueva aproximaci√≥n mediante: `p = p1 - q1 * (p1 - p0) / (q1 - q0)`.

  - No garantiza que las aproximaciones encuadren una ra√≠z. Esto puede hacer que la convergencia sea r√°pida, pero en algunos casos puede perder la garant√≠a de que la ra√≠z se encuentra entre las aproximaciones.

- **M√©todo de la Falsa Posici√≥n**  
  - Tambi√©n usa la misma f√≥rmula: `p = p1 - q1 * (p1 - p0) / (q1 - q0)`.  
  - Sin embargo, **mantiene siempre un intervalo `[p0, p1]` donde `f(p0)` y `f(p1)` tienen signos opuestos**. Despu√©s de calcular `p`, se decide qu√© extremo actualizar seg√∫n el signo de `f(p)`:
    - Si `f(p)` tiene el mismo signo que `f(p1)`, se actualiza `p0` (y se conserva el bracketing).
    - Si `f(p)` tiene el mismo signo que `f(p0)`, se actualiza `p1`.
  - De esta forma, se garantiza que en cada iteraci√≥n el intervalo sigue conteniendo una ra√≠z.

### Ejemplo de Uso

Consideremos la funci√≥n `f(x)=x^2-3`, cuya ra√≠z real es `sqrt{3} \approx 1.73205`. Utilizaremos las aproximaciones iniciales `p0=1.0` y `p1=2.0`.

```python

def f(x):
    return x**2 - 3

p0 = 1.0
p1 = 2.0

solution, iterations = false_position(f, p0, p1, TOL=1e-5, N0=100)
print(f"La soluci√≥n encontrada es {solution} en {iterations} iteraciones.")
```

## 6- M√©todo de Steffensen (`steffensen_method(g, p0, TOL=1e-5, N0=100)`)

El M√©todo de Steffensen es un m√©todo para acelerar la convergencia de una iteraci√≥n de punto fijo. Es decir, se utiliza para encontrar la soluci√≥n de `p = g(p)` a partir de una aproximaci√≥n inicial. La idea es aplicar el proceso de aceleraci√≥n de Aitken para obtener una mejor aproximaci√≥n en cada iteraci√≥n.

### Proceso del M√©todo

1. **Conversi√≥n**  
   Se convierte la funci√≥n dada a la forma `p = g(p)`.

2. **Inicializaci√≥n**

   Se establece la aproximaci√≥n inicial y se fija el contador de iteraciones en `i = 1`.

3. **Iteraci√≥n**

   Se calcula `p1 = g(p0)` (se eval√∫a la funci√≥n en `p0`).
   Se verifica `p1`, si es igual (o pr√°cticamente igual) a `p0` ya hemos convergido.
   Se calcula `p2 = g(p1)` (se eval√∫a la funci√≥n en `p1`).
   Se calcula la nueva aproximaci√≥n utilizando la f√≥rmula `p = p0 - ((p1 - p0) ** 2) / (p2 - 2 * p1 + p0)`.
   Se verifica el criterio de convergencia.
   Si no se cumple el criterio de convergencia, se incrementa `i` en 1 y se actualiza `p0` con el valor de `p` para proceder a la siguiente iteraci√≥n.

4. **Fallo**

   Si se alcanza el n√∫mero m√°ximo de iteraciones `N0` sin que se cumpla el criterio de convergencia, se lanza un error indicando que el m√©todo no fue exitoso.

### Parametros de Entrada y Salida

```python
def steffensen_method(g, p0, TOL=1e-5, N0=100):
    """
    Entradas:
       g    : funci√≥n (callable) que define el m√©todo de punto fijo, es decir, g(p).
       p0   : aproximaci√≥n inicial.
       TOL  : tolerancia para la convergencia.
       N0   : n√∫mero m√°ximo de iteraciones permitidas.

    Salida:
       Retorna una tupla (p, i) donde:
          - p es la aproximaci√≥n a la soluci√≥n.
          - i es el n√∫mero de iteraciones realizadas.
    """
``` 

### Ejemplo de Uso

Supongamos que queremos resolver la ecuaci√≥n de punto fijo para la funci√≥n `g(x)=cos(x)`

```python

def g(x):
    return math.cos(x)

p0 = 1.0 

solution, iterations = steffensen_method(g, p0, TOL=1e-5, N0=100)
print(f"La soluci√≥n encontrada es {solution} en {iterations} iteraciones.")
```

## 6- M√©todo de Honer (`horner_method(a, x0)`)

El **M√©todo de Horner** es una forma eficiente de evaluar un polinomio en un valor espec√≠fico `x0`. Adem√°s, puede adaptarse para obtener tambi√©n la derivada del polinomio en ese mismo punto. Dado un polinomio de grado `n`:

 `P(x) = a_n*x^n + + a_{n-1}*x^{n-1} + ... + a1*x + a0`

Queremos calcular:
   - `y = P(x0)`
   - `z = P'(x0)`

### Proceso del M√©todo

Para evaluar un polinomio y su derivada en un punto `x_0` usando el m√©todo de Horner, se asume que el polinomio se expresa en la forma `P(x) = a_n*x^n + + a_{n-1}*x^{n-1} + ... + a1*x + a0`

Se asume que los coeficientes se proporcionan en orden descendente, es decir, `a = [a_n, a_(n-1), ..., a1, a0]` donde `a_n` es el coeficiente del t√©rmino de mayor grado y `a0` el t√©rmino independiente.

1. **Entrada**

   Se recibe:
     - La lista de coeficientes en orden descendente.
     - El punto de evaluaci√≥n `x0`.

2. **Inicializaci√≥n**

   Se define:
     - El grado `n` (impl√≠cito en la longitud de la lista de coeficientes).
     - `y = a0` (inicialmente, el acumulador para `P(x0)` es el coeficiente del t√©rmino de mayor grado).
     - `z = a0` (inicialmente, el acumulador para `P'(x0)` es el mismo coeficiente).

3. **Iteraci√≥n**

   Se van "desplazando" los coeficientes y acumulando el valor del polinomio y de su derivada de manera eficiente.

4. **Finalizaci√≥n**

   Al finalizar, `y` contiene `P(x0)` y `z` contiene `P'(x0)`.

5. **Salida**

   Se retorna el par `(y, z)`.

### Parametros de Entrada y Salida

```python
def horner_method(a, x0):
    """    
    Entradas:
      a   : lista de coeficientes [a_n, a_(n-1), ..., a_1, a_0].
      x0  : punto en el que se quiere evaluar el polinomio y su derivada.
    
    Salida:
      (y, z) donde:
         y = P(x0)   (valor del polinomio en x0)
         z = P'(x0)  (valor de la derivada en x0)
    """
``` 

### Ejemplo de Uso

Supongamos que queremos resolver la ecuaci√≥n P(x) = 2x^3 - 6x + 4

```python

if __name__ == "__main__":
    # Polinomio: P(x) = 2x^3 - 6x + 4
    # Coeficientes en orden descendente: [2, 0, -6, 4]
    a = [2, 0, -6, 4]
    x0 = 4

    y, z = horner_method(a, x0)
    print(f"P({x0}) = {y}")
    print(f"P'({x0}) = {z}")
```

## 7- M√©todo de M√ºller

El **M√©todo de M√ºller** es una t√©cnica num√©rica para encontrar ra√≠ces de ecuaciones no lineales, que es una generalizaci√≥n del m√©todo de la secante y puede usar aritm√©tica compleja para manejar funciones donde otras t√©cnicas no convergen. Se destaca por su capacidad para alcanzar r√°pidamente la convergencia incluso con aproximaciones iniciales relativamente lejanas de la ra√≠z, y es √∫til especialmente cuando se desconocen derivadas de la funci√≥n o son dif√≠ciles de calcular.

El m√©todo utiliza tres aproximaciones iniciales `p0, p1, p2` y a trav√©s de un proceso iterativo trata de encontrar una ra√≠z ùëù de la funci√≥n `f(x)`. Utiliza diferencias divididas y una forma cuadr√°tica para estimar la siguiente aproximaci√≥n.

### Proceso de M√©todo

1. **Elecci√≥n de Puntos Iniciales** 

   Se seleccionan tres puntos iniciales, `p0, p1, p2`, que son necesarios para comenzar el algoritmo. Estos puntos deben ser elegidos cuidadosamente para asegurar que est√©n cerca de la ra√≠z que se desea encontrar.

2. **Inicializaci√≥n**

   Se calculan las diferencias `h1 = p1 - p0` y `h2 = p2 - p1`.
   Se determinan las diferencias divididas `Œ¥1 = (f(p1) - f(p0)) / h1` y `Œ¥2 = (f(p2) - f(p1)) / h2`.
   Se calcula la derivada dividida de segundo orden `d = (Œ¥2 - Œ¥1) / (h2 + h1)`.
   Se establece el contador de iteraciones en 3.

3. **Iteraci√≥n**

   En cada iteraci√≥n se realiza lo siguiente:
     - Se calcula `b = Œ¥2 + h2 * d`.
     - Se determina el discriminante `D = (b**2 - 4 * f(p2) * d) ** 0.5`.
     - Se elige `E` de manera que `E = b + D` si `|b - D| < |b + D|`, de lo contrario `E = b - D`.
     - Se calcula el paso hacia la nueva aproximaci√≥n `h = -2 * f(p2) / E`.
     - Se actualiza `p = p2 + h`.

4. **Evaluaci√≥n de Convergencia**

   Se verifica si el tama√±o del paso `|h|` es menor que la tolerancia. Si es as√≠, se retorna `p` como la ra√≠z encontrada y se termina el proceso.

5. **Actualizaci√≥n para la Siguiente Iteraci√≥n**

   Se actualizan los puntos: `p0 = p1`, `p1 = p2`, `p2 = p`.
   Se recalculan las diferencias y diferencias divididas para la pr√≥xima iteraci√≥n.
   Se incrementa el contador de iteraciones.

6. **Condici√≥n de Fallo**

   Si se alcanzan `N0` iteraciones sin que la diferencia `|h|` alcance la tolerancia, se lanza un error indicando que el m√©todo no ha sido exitoso despu√©s de `N0` iteraciones.

### Parametros de Entrada y Salida

```python
def muller(f, p0, p1, p2, TOL=1e-5, N0=100):
    """
    Entradas:
      f   : funci√≥n (callable) para la cual se busca la ra√≠z, f(x)=0.
      p0  : primera aproximaci√≥n inicial.
      p1  : segunda aproximaci√≥n inicial.
      p2  : tercera aproximaci√≥n inicial.
      TOL : tolerancia para la convergencia, predeterminada a 1e-5.
      N0  : n√∫mero m√°ximo de iteraciones permitidas, predeterminado a 100.
    
    Salida:
      p   : aproximaci√≥n a la ra√≠z de la funci√≥n o error si no se encuentra.
    """
```

### Ejemplo de Uso

```python
def f(x):
    return x**2 - 3*x + 2

p0 = 3
p1 = 4
p2 = 5

root = muller_method(f, p0, p1, p2, TOL=1e-5, N0=100)
print(f"La ra√≠z encontrada es: {root}")
```




## Adicional

### M√©todo privado para derivar `(_derivative(f, TOL=1e-5))`
Metodo privado que retorna una funci√≥n que aproxima la derivada de `f` utilizando la f√≥rmula de diferencia centrada.

**Par√°metros**:
   - **f** : Funci√≥n (callable) de la cual se quiere calcular la derivada.
   - **TOL** : Paso peque√±o para la aproximaci√≥n (por defecto 1e-5).